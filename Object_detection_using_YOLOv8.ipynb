{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**OBJECT DETECTION USING YOLOv8**"
      ],
      "metadata": {
        "id": "7Vp3C6gUt4pq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective:**\n",
        "\n",
        "The goal of this project is to perform real-time object detection on a 30-second dashboard camera view video using YOLOv8 mode.\n",
        "\n",
        "This system identifies and localizes road objects—such as trucks, buses,Person and other vehicles—by drawing bounding boxes and adding class labels directly onto the video frames.\n",
        "\n",
        "This project showcases an industry-standard, deep learning-based solution for traffic surveillance and smart mobility applications"
      ],
      "metadata": {
        "id": "TA8Lb5NjuStv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3U2xViZjJHE",
        "outputId": "deba03f1-cada-487f-b947-5eec858eeb5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Installing Ultralytics Framework (YOLOv8)\n",
        "!pip install ultralytics --quiet\n",
        "\n",
        "# Import Required Libraries\n",
        "from ultralytics import YOLO\n",
        "from google.colab import drive # Due to memory constraints i've used my Google Drive to access the files\n",
        "import cv2,os\n",
        "from IPython.display import Video, display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mouning Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RUea_8VsyUON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a path to access the file which im going to annotate\n",
        "input_video_path = \"/content/drive/MyDrive/PROJECT/video1.mp4\""
      ],
      "metadata": {
        "id": "yjNbybBxkMhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a path to store my output\n",
        "output_dir = \"/content/drive/MyDrive/PROJECT\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "output_video = os.path.join(output_dir, \"annotated_video.mp4\")"
      ],
      "metadata": {
        "id": "3Ew_2JEvlA1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLOv8 model\n",
        "model = YOLO(\"yolov8n.pt\") #Used YOLOv8n (nano version) due to RAM constraints. We can also use yolov8m.pt for better accuracy if system resources allow."
      ],
      "metadata": {
        "id": "DY5VxVW3lZPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open video reader and writer\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))"
      ],
      "metadata": {
        "id": "8UtUAbUJlfj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Streaming inference: low memory usage\n",
        "results = model.predict(source=input_video_path, stream=True)\n",
        "\n",
        "for r in results:  # each r is a single frame result\n",
        "    frame = r.orig_img  # original frame as numpy array\n",
        "    boxes = r.boxes.xyxy.numpy()  # bounding box coordinates: [x1, y1, x2, y2]\n",
        "    classes = r.boxes.cls.numpy().astype(int)  # detected class indices\n",
        "    scores = r.boxes.conf.numpy()  # confidence scores\n",
        "\n",
        "    # Overlay bounding boxes and labels\n",
        "    for (x1, y1, x2, y2), cls, conf in zip(boxes, classes, scores):\n",
        "        label = f\"{model.names[cls]} {conf:.2f}\"\n",
        "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,255,0), 2)\n",
        "        cv2.putText(frame, label, (int(x1), int(y1)-10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n",
        "\n",
        "    out.write(frame)  # save annotated frame\n",
        "\n",
        "cap.release()\n",
        "out.release()"
      ],
      "metadata": {
        "id": "QYlBhsNY0Vnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display result\n",
        "display(Video(output_video, embed=True)) # The Annoted video automatically saved in our output path which we have chosen."
      ],
      "metadata": {
        "id": "n0qLZInmmDD_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}